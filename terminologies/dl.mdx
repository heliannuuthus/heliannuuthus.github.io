---
slug: neuron
title: 神经元
authors: [heliannuuthus]
description: 神经元是深度学习中的基本单元，通常由输入、权重矩阵、偏置向量和激活函数组成。本质是执行线性计算，并将其映射到非线性空间。
---

> 神经元是深度学习中的基本单元，通常由输入、权重矩阵、偏置向量和激活函数组成。本质是执行线性计算，并将其映射到非线性空间。

输入向量通过权重矩阵进行加权计算，然后加上向量偏置，完成线性计算，最后通过激活函数将计算结果映射到非线性空间。

常规的 3 层神经网络结构如下：

1. 输入向量 $x$ 通过第一层权重矩阵 $W_1$ 进行加权计算，然后加上第一层偏置向量 $b_1$，得到第一层输出向量 $h_1$。
2. 第一层输出向量 $h_1$ 通过第二层权重矩阵 $W_2$ 进行加权计算，然后加上第二层偏置向量 $b_2$，得到第二层输出向量 $h_2$。
3. 第二层输出向量 $h_2$ 通过激活函数 $f$ 进行非线性计算，得到最终输出向量 $y$。

$$
y = f(W_2 h_2 + b_2)
$$

其中，$W_1$ 是第一层权重矩阵，$b_1$ 是第一层偏置向量，$W_2$ 是第二层权重矩阵，$b_2$ 是第二层偏置向量，$f$ 是激活函数。

---

---

slug: activation-function
title: 激活函数
authors: [heliannuuthus]
description: 激活函数是神经元中的非线性函数，用于将线性计算结果映射到非线性空间。

---

> 激活函数是神经元中的非线性函数，用于将线性计算结果映射到非线性空间。

常见的激活函数有：

- [ReLU](<https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>)
- [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)
- [Tanh](https://en.wikipedia.org/wiki/Hyperbolic_tangent)
- [Softmax](https://en.wikipedia.org/wiki/Softmax_function)
