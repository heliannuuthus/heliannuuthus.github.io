---
slug: prompt-engineering
title: 提示词工程导览
authors: ["heliannuuthus"]
tags: ["AI"]
draft: true
---

最近在做关于智能体系统的 AI 知识学习、智能体知识库构建和提示词工程。
受 AI 平台 Prompt 产品交流影响和启发，也却身体会到了自己写的 Agent Prompt 会让 LLM 像个 dinner，所以打算系统性的学习一下 Prompt Engineering。
本章节受 [Prompt Engineering Guide](https://www.promptingguide.ai/) 启发，并结合自己的理解进行整理。

<!--truncate-->

## 提示词要素

常规的提示词通常包含以下要素：

- **指令（instruction）**：想要模型执行的指令，简洁和直接，通常用一或几句话来概括。
- **上下文（context）**：一些外部知识或前置知识的上下文信息，引导 :ctip[LM]{#语言模型} 更好的响应用户请求。
- **输入数据（input）**：用户输入的问题和内容。
- **输出数据（output）**：大模型输出的答案，当前大部分 LM 都具备结构化输出的能力。

### 指令

> 并不存在任何特定 :ctip[tokens]{#词元或关键字} 能为 LM 的回答带来很积极的作用
> 
- **结构化**：结构化的格式和详细的描述能使 LM 对问题和背景理解得更为透彻
- **有效性**：指令应该更明确，意图更清晰直接，保证信息传达的有效
- **注意力**：集中 LM 注意力，要告诉 LM 做什么而不是告诉它**不做什么**

### 上下文

> LM 回答的准确性和上下文强相关，幻觉是 LM 的既存问题，好的上下文能极大程度上缓解它
> 
- 上下文可以是 :ctip[Q&A]{id="Question and Answer"} 对，也能是一篇文章、一个背景故事、一个技术方案等
- 上下文不是必须的，LM 是对“世界”存在有限的感知能力，所以我们也需要严令禁止 LM 瞎说

## 提示词技术

:::nerd
AI 从早几年前就已经开始发酵，到 OpenAI 问世算是彻底爆发，开启 AI 元年。其间 Prompt 技术的演化历史也十分的精彩
:::

本质是引导 LM 正确且准确的完成任务，算是“教会” LM 一种学习方法来更好地应对接下来的问题

### 零样本提示词

> 零样本提示词（Zero Shot Prompt）指 LM 在几乎没有任何的:ctip[上下文]{#context}环境中对:ctip[用户输入]{#input}进行分析学习，最后给出答案
> 
1. 通常用于执行情感分析或者输入较为明确的**单一意图识别任务**
2. LM 的 Large 来源于庞大的训练数据，所以它有超强的零样本能力

### 少样本提示词

> 少样本提示词（Few Shot Prompt）指填充少量的 Q&A 对作为样本填充在 LM Prompt 的上下文以引导 LM 进行类似问题或行为分析
> 
1. LM 通常会按照引入的少量 samples 的简单思维结构来回答用户的问题，加上幻觉，其实能很清晰的感知到，它们并不能识别一些 samples 之外的原则性的错误
2. 在处理复杂的，甚至涉及一些推理步骤的问题时，少样本提示词作用在 LM 上的结果会愈发地显得诡异

### 链式思考（思维链）

> 思维链（Chain of Thought）通过向 Prompt 加入中间推理过程强制 LM 进行思考，本质是一种 Feedback，达到修正自身 CoT 的目的
> 
1. 结合少量样本提示，可提升被单一的少量样本提示限制的推理能力
2. 结合零样本提示，使用 LM 自身对物理世界构建的 CoT 进行问题推理，也是一种很棒的方式
3. *Auto-CoT* 是一种人工消除 LM 在推理过程中产生的错误的手段，主要分为两步：
    1. 问题分类：面临的问题是可分类的且是有限的
    2. 抽样表达：针对每一类问题，都能找到其中具有代表性的一个，并且为 LM 演示 CoT 构建过程，可使用简单的规则对 LM 进行启发，达到动态构建 CoT 的目的（例如，输入的长度和推理的步骤正相关）

### 自我一致性

> 自我一致性（Self-Consistency）比 CoT 更稳定，CoT 容易在某一步推理出错导致雪崩，稳定性并不高。\
而自 SC 构造多条 CoT 进行多路径总结通过投票机制得到更权威的正确答案
> 
1. SC 与问题推理出正确答案的多条 CoT 输出内容强相关
2. 本质利用 CoT 通过调整 LLMs :ctip[temperature]{id="温度通常大于 0.5"} 和 :ctip[top_p]{id="核采样通常小于 0.9"} 构造 10+ 条答案，加以多数表决机制，得到更权威的正确答案

### 生成知识提示词

> 生成知识提示词（Generated Knowledge Prompt）指出 LM 可以在检索现有知识库之后、预测之前，对已有知识或单纯的围绕着问题并结合少样本提示进行知识提示词的自生成，达到提升常识性推理性能的目的
> 
1. 通过少量样本提示 LM 生成与问题相关的知识陈述
    1. 提示设计包含任务说明，少量手写的问题以及知识示例
    2. 生成参数使用核采样，每条问题生成 20 条知识（不同的 LM 可能不同），并对期进行置信度评分
2. 影响性能和结果的因素分为三点：
    1. 知识质量事实正确性较高，并且对推理有较积极的作用
    2. 知识数量在 20 条时开始饱和，过多会因噪声下降
    3. 知识整合：“选择最佳知识策略” 比 MoE（专家混合）和 PoE（专家乘积）效果更好
3. **选择最佳知识策略**
    1. 生成知识陈述：根据特定任务的少量样本提示词生成与问题相关的知识陈述。
    2. 构建知识增强问题：将生成的每个知识陈述与原始问题进行拼接，形成多个知识增强的问题
    3. 计算答案选择聚合分数：使用推理模型计算每个答案在不同知识增强问题下的支持度分数，最终聚合分数由使答案获得最高支持度的知识陈述对应的分数决定
    4. 将聚合分数最高的答案为预测结果

### 链式提示词

> 链式提示词（prompt chaining）本质是将一个重要的提示词技术拆分成多个子任务，将上一个子任务的输出作为下一个子任务的 Prompt 提供给 LLM（Feedback），以提升 LLM 性能并保证其输出是稳定且透明的
> 
1. 通常中间子任务的输出是结构化的，并且前后两个子任务有一定的关联性，譬如精简语句、总结摘要和信息提取等
2. 链式提示词对 LLM 的上下文长度有一定的要求，往往最后一个子任务的 Prompt 会结合前面所有子任务的输出
3. 通过这种链式输出，可以很方便的定位到 LLM 在整个流程中犯的错

### 思维树

> 思维树（Tree of Thought）维护着多条连贯的思维链，本质是缓解 LM 受限于 Token-Level 和从左到右决策在生成内容时带来的消极影响。
> 
- LM 生成内容时，是从左到右的一个一个 token 去生成的，并且下一个 token 是基于上一个 token 预测的。将 LM 本身的前瞻性和计算流程全部压缩到了这一个 token。当 token 出现误判可能会导致雪崩。
- 思维树的构建过程可以通过 Prompt 实现，也能通过:term[强化学习]{./terms/dl#reinforcement-learning}实现
    - 通过强化学习实现的思维树，在新环境上更具有鲁棒性
    - 通过 Prompt 实现的思维树，在推理过程中更具有可解释性

```markmap
## 问题分解
- 识别问题，确认问题范围
- 定义节点职责
- 约束节点连通条件
## 思维生成
- 节点根据当前解答状态生成多条思维
- 不同任务类型，定义的节点不同，思维生成方式也会不一样
  - 零样本或者少量样本 + CoT Prompt
  - 根据引导性 Prompt 顺序生成
## 思维评估
- 评估结果不理想，应回溯到**父节点**重新生成思维
- 多次评估不理想，直接回溯到**祖节点**重新生成思维
- 使用*计数器*，防止多次进入错误节点引发思维爆炸
- 思维评估的指标，根据任务类型不同，指标也会不一样
  - `数学问题`：答案的正确性
  - `常识问题`：答案的合理性
  - `推理问题`：答案的逻辑性

## 思维选择
- 通常使用二维矩阵存储整棵思维树
  - 一行标识单条思维链
  - 多列表示当前结果
- 搜索算法（列举了部分，但并不全面）
  - 广度优先搜索（BFS）：用于问题子步骤较少，保留潜力较大的节点
  - 深度优先搜索（DFS）：用于复杂的搜索任务，积极选择潜力较大的节点，状态无解需要回溯
  - 启发式搜索（A*）：选择局部潜力最大的节点优先进行搜索，较 DFS 更高效，容易陷入局部最优
  - 模拟退火（SA）：接受更差的情况，但有概率接受更优的情况，避免陷入局部最优
```

### 检索增强生成

> 检索增强生成（RAG）是一种结合了实时检索增强 LM 与现实环境交互感知的提示词技术。\
在 2020 年 Meta AI 提出 RAG 模型，使用 seq2seq 生成模型结合外部知识检索组件，组合输入 + 检索文档生成输出。\
当下 LLMs 拥有极强的上下文记忆能力，并且在内存参数中存储了大量知识，RAG 就检索组件 + seq2seq 生成模型转换成检索组件 + 通用 LLM 实现。
> 
- RAG 本质就是把信息检索组件和 LM 组合在一起，对信息检索组件进行参数或答案的微调，可以看作是对 RAG 的微调
- 可参考传统 seq2seq 模型，将检索的文档和用户输入进行拼接，让 LM 生成的输出效果更佳
- 对于准确性要求较高的场景，可通过 Prompt 强制约束 LLMs 使用检索组件的返回值，降低内置参数数据集的影响权重

### 自动推理和工具使用

> 自动推理和工具使用（Auto-Reasoning and Tool Use）通过预制任务库的范例，让 LLMs 在执行任务时，\
参考范例在应用层自动生成推理子步骤并且在需要使用工具的地方调用工具，最后对输出进行整合。
> 
- 需要**预制任务库**，并且对其有较为明确的约束
- 其中任务需要使用**特定语法**进行约束，让 LLM 能更好的理解并泛化
- LM 生成的推理子步骤是可见的，可以进行**人工干预**达到更好的效果

```markmap
## 任务库
- 任务多样
- 格式规范
- 内容准确
- 可解释性强

## 工具使用
- 通用工具
  - 在线搜索
  - 数学计算
  - 翻译
  - ...
- 定制工具
  - MCP Server
  - 代码生成
  - 沙箱环境
  - ...
```

### 自动提示词工程师

> 自动提示词工程师（Automated Prompt Engineer）通过 LLMs 自动生成和选择高质量 Prompt，以替代人工构建 Prompt。\
针对原始零样本或少量样本场景强依赖人为构建，而 LLMs 和人类理解环境差异较大，通过 LLM 自动构建 Prompt 达到优化的目的。

- **指令筛选**：$LLM_A$ 通过初始提案对自然语言指令集进行筛选
   - 正向生成：提供 Q&A 对，让 $LLM_A$ 推测更具回复能力和信息熵的指令
   - 反向生成：用支持文本填充的 LLM 全集填充，记录所有预填充的提示词
   - 定制提示词：针对特定人物提供特定提案模板

- **指令评估**：$LLM_A$ 通过量化提示效果来达到筛选的目的
   - 执行准确率：使用 Q&A 评估执行准确率，适合分类/生成任务等
   - 对数概率：利用信息量评估提示词效果，适合低质量评估筛选
   - 高效评估策略：通过多阶段不同样本类型进行评估
      1. **小样本通用评估**：通过*少量样本*评估全部模型，提取高分模型
      2. **新样本高分评估**：利用*新样本*评估高分模型
      3. **全样本验证**：最后使用*全量样本*验证高分评估的高分模型

### 激活提示词

> 激活提示词（Activated Prompt）解决传统 CoT 构造依赖**人工标注示例、推理步骤**并且对不同的任务适应性差的问题。\
借鉴不确定主动学习思想，对特定任务池进行**不确定性评估**，选取 LLMs 最不确定的样本进行人工标注推理步骤和答案。


- **不确定性评估**：选择规模 \<\= 1000 的样本中的每个问题调用 LLM 生成 k 条答案（包含推理步骤），有以下量化指标：
   - 熵：基于答案的频次计算得到，$H(X)=-\sum_{i=1}^{n} p(x_i) \log_2 p(x_i)$
   - 分歧：$u=\frac{\text{unique answers}}{k} $
   - 方差：$v=\frac{1}{k} \sum_{i=1}^{k} (x_i - \bar{x})^2$
   - 需要排除自置信（LLM 对自身答案的置信度较高）
- **高确定性问题选择**：按不确定性降序排序，选择 **top-n** 的问题，选择面如果较多可使用随机采样
- **人工标注**：重点在于选择，标注的质量权重不高
- **推理**：将示例集前置到每个测试问题前，采用 SC 进行推理，选择合适的温度生成 n 次答案选择最优答案。可用 "Let's think step by step" 触发 LLM 生成步骤的关键词

### 定向刺激性提示词

> 方向性刺激提示词（Directional Stimulus Prompting）旨在利用小型可调整策略模型来引导黑盒大模型生成特定目标输出。\
避免直接调优 LLM 的高成本与不可行性。

- 小型可调整策略模型（T5 等）为每个输入实例生成实例:ctip[特定的定向刺激提示]{id="摘要任务的关键词、对话任务的对话行为以及推理任务的 CoT 触发语"}，并可通过以下两种方式进行微调：
  1. 基于少量标注数据的:term[监督微调]{./terms/dl#supervised-fine-tuning}
  2. 基于 LLM 输出奖励的:term[强化学习]{./terms/dl#reinforcement-learning}


### ReAct 框架
> ReAct（Reasoning Action）以交错的方式生成推理轨迹（Reasoning Trajectory）和任务特定动作（Task-Specific Actions）。\
推理轨迹帮助 LLMs 生成、追踪和调整行动方案并处理异常，行动则让 LLMs 与外部工具交互获取信息以解决问题

- 利用 Few Shots（1-6 个）输入人类标注的“动作-推理-观察”轨迹示例，随后让 LLMs 自己生成轨迹
  - 推理密集型任务（如数学）：严格按照“动作-推理-观察”步骤执行
  - 非推理密集型任务（如摘要）：可由 LLMs 自行插入推理轨迹